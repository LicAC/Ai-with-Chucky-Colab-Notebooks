{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AICHUCKY/Ai-with-Chucky-Colab-Notebooks/blob/main/Z_Image_Turbo_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Header_Cell"
      },
      "source": [
        "<div style=\"background-color: #161b22; padding: 20px; border-radius: 12px; border-left: 6px solid #2ecc71; box-shadow: 0 4px 6px rgba(0,0,0,0.3);\">\n",
        "  <h1 style=\"color: #ffffff; margin: 0; font-family: sans-serif; font-weight: 700; letter-spacing: 1px;\">\n",
        "    ‚ö° Z-Image Turbo <span style=\"font-size: 0.6em; color: #2ecc71; vertical-align: middle; background: #2ecc7122; padding: 2px 8px; border-radius: 6px;\">Pro</span>\n",
        "  </h1>\n",
        "  <p style=\"color: #8b949e; margin: 8px 0 0 0; font-family: sans-serif;\">\n",
        "    Next-Gen FP8 Diffusion ‚Ä¢ ComfyUI Backend ‚Ä¢ Smart Caching\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Setup_Cell"
      },
      "outputs": [],
      "source": [
        "#@title 1. Initialize Environment\n",
        "#@markdown This step installs dependencies and downloads models. <br>\n",
        "#@markdown **Note:** Files are cached, so subsequent runs will be instant.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# --- UI Logger ---\n",
        "def log(msg, type=\"info\"):\n",
        "    colors = {\n",
        "        \"info\": \"\\033[94m\",  # Blue\n",
        "        \"success\": \"\\033[92m\", # Green\n",
        "        \"warn\": \"\\033[93m\",  # Yellow\n",
        "        \"error\": \"\\033[91m\", # Red\n",
        "        \"reset\": \"\\033[0m\"\n",
        "    }\n",
        "    symbols = {\n",
        "        \"info\": \"‚ûú\",\n",
        "        \"success\": \"‚úì\",\n",
        "        \"warn\": \"‚ö†\",\n",
        "        \"error\": \"‚úó\"\n",
        "    }\n",
        "    print(f\"{colors[type]}{symbols[type]} {msg}{colors['reset']}\")\n",
        "\n",
        "def run_quiet(cmd, desc):\n",
        "    print(f\"   ‚è≥ {desc}...\", end=\"\\r\")\n",
        "    try:\n",
        "        subprocess.check_call(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        print(f\"\\033[92m   ‚úì {desc}\\033[0m    \")\n",
        "    except Exception as e:\n",
        "        print(f\"\\033[91m   ‚úó {desc} Failed\\033[0m\")\n",
        "        print(e)\n",
        "\n",
        "# --- Paths ---\n",
        "ROOT = Path(\"/content\")\n",
        "COMFY_PATH = ROOT / \"ComfyUI\"\n",
        "MODELS_PATH = COMFY_PATH / \"models\"\n",
        "\n",
        "log(\"Checking Environment...\", \"info\")\n",
        "\n",
        "# 1. Install ComfyUI\n",
        "if not COMFY_PATH.exists():\n",
        "    run_quiet(\"git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\", \"Cloning Core Repo\")\n",
        "    run_quiet(\"pip install -r /content/ComfyUI/requirements.txt\", \"Installing Python Dependencies\")\n",
        "else:\n",
        "    log(\"ComfyUI Core found\", \"success\")\n",
        "\n",
        "# 2. Install Tools\n",
        "run_quiet(\"apt -y install -qq aria2\", \"Installing Accelerator (Aria2)\")\n",
        "\n",
        "# 3. Download Models\n",
        "model_map = [\n",
        "    (\"https://huggingface.co/T5B/Z-Image-Turbo-FP8/resolve/main/z-image-turbo-fp8-e4m3fn.safetensors\",\n",
        "     MODELS_PATH / \"diffusion_models\" / \"z-image-turbo-fp8-e4m3fn.safetensors\", \"Turbo Checkpoint\"),\n",
        "    (\"https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/text_encoders/qwen_3_4b.safetensors\",\n",
        "     MODELS_PATH / \"clip\" / \"qwen_3_4b.safetensors\", \"Qwen Encoder\"),\n",
        "    (\"https://huggingface.co/Comfy-Org/z_image_turbo/resolve/main/split_files/vae/ae.safetensors\",\n",
        "     MODELS_PATH / \"vae\" / \"ae.safetensors\", \"VAE Decoder\")\n",
        "]\n",
        "\n",
        "log(\"Verifying Models...\", \"info\")\n",
        "for url, path, name in model_map:\n",
        "    if not path.exists():\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        run_quiet(f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M '{url}' -d '{path.parent}' -o '{path.name}'\", f\"Downloading {name}\")\n",
        "    else:\n",
        "        log(f\"{name} is ready\", \"success\")\n",
        "\n",
        "log(\"Setup Complete! Please load the engine below.\", \"success\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Load_Cell"
      },
      "outputs": [],
      "source": [
        "#@title 2. Load Engine\n",
        "#@markdown Loads the AI models into GPU memory. This takes about 30 seconds.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import re, uuid, gc\n",
        "\n",
        "# Add path\n",
        "sys.path.append(str(COMFY_PATH))\n",
        "\n",
        "log(\"Booting ComfyUI Backend...\", \"info\")\n",
        "\n",
        "try:\n",
        "    from nodes import NODE_CLASS_MAPPINGS\n",
        "except ImportError:\n",
        "    log(\"ComfyUI not found. Please run Step 1.\", \"error\")\n",
        "    raise SystemExit\n",
        "\n",
        "# Load Nodes\n",
        "with torch.inference_mode():\n",
        "    nodes = {\n",
        "        \"unet\": NODE_CLASS_MAPPINGS[\"UNETLoader\"](),\n",
        "        \"clip\": NODE_CLASS_MAPPINGS[\"CLIPLoader\"](),\n",
        "        \"vae\": NODE_CLASS_MAPPINGS[\"VAELoader\"](),\n",
        "        \"enc\": NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"](),\n",
        "        \"sampler\": NODE_CLASS_MAPPINGS[\"KSampler\"](),\n",
        "        \"decode\": NODE_CLASS_MAPPINGS[\"VAEDecode\"](),\n",
        "        \"empty\": NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "    }\n",
        "\n",
        "    print(\"   ‚è≥ Loading Checkpoints into VRAM...\", end=\"\\r\")\n",
        "    unet_model = nodes[\"unet\"].load_unet(\"z-image-turbo-fp8-e4m3fn.safetensors\", \"fp8_e4m3fn_fast\")[0]\n",
        "    clip_model = nodes[\"clip\"].load_clip(\"qwen_3_4b.safetensors\", type=\"lumina2\")[0]\n",
        "    vae_model = nodes[\"vae\"].load_vae(\"ae.safetensors\")[0]\n",
        "\n",
        "    log(\"Engine Online. Ready to Generate.\", \"success\")\n",
        "\n",
        "# Utils\n",
        "SAVE_DIR = ROOT / \"results\"\n",
        "SAVE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def get_save_path(prompt):\n",
        "    clean_promt = re.sub(r'[^a-zA-Z0-9_-]', '_', prompt)[:20]\n",
        "    filename = f\"{clean_promt}_{uuid.uuid4().hex[:4]}.png\"\n",
        "    return SAVE_DIR / filename\n",
        "\n",
        "def flush_mem():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Gen_Cell"
      },
      "outputs": [],
      "source": [
        "#@title 3. Creator Studio\n",
        "#@markdown Enter your prompt and settings to generate images.\n",
        "\n",
        "#@markdown ### üé® **Prompting**\n",
        "positive_prompt = \"A cinematic shot of a futuristic neon city, rain reflections, cybernetic aesthetics, 8k masterpiece\" # @param {type:\"string\"}\n",
        "negative_prompt = \"blurry, low quality, text, watermark, distorted\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ### üìê **Dimensions & Quality**\n",
        "aspect_ratio = \"1280x720 (16:9 Landscape)\" # @param [\"1024x1024 (1:1 Square)\", \"1280x720 (16:9 Landscape)\", \"720x1280 (9:16 Portrait)\", \"1152x864 (4:3 Photo)\", \"1344x576 (21:9 Cinema)\"]\n",
        "steps = 20 # @param {type:\"slider\", min:10, max:50, step:1}\n",
        "guidance_scale = 1 # @param {type:\"slider\", min:1.0, max:10.0, step:0.5}\n",
        "\n",
        "#@markdown ### ‚öôÔ∏è **Advanced**\n",
        "seed = -1 # @param {type:\"number\"}\n",
        "auto_download = False # @param {type:\"boolean\"}\n",
        "\n",
        "# Parse Inputs\n",
        "w, h = [int(x) for x in aspect_ratio.split(\"(\")[0].strip().split(\"x\")]\n",
        "gen_seed = torch.randint(0, 2**63 - 1, (1,)).item() if seed == -1 else seed\n",
        "\n",
        "try:\n",
        "    flush_mem()\n",
        "    print(f\"\\033[94m‚ûú Generating: {w}x{h} | Steps: {steps} | Seed: {gen_seed}\\033[0m\")\n",
        "\n",
        "    # IMPORTANT: Use inference_mode to avoid tensor version errors during decoding\n",
        "    with torch.inference_mode():\n",
        "        # Encode\n",
        "        pos_enc = nodes[\"enc\"].encode(clip_model, positive_prompt)[0]\n",
        "        neg_enc = nodes[\"enc\"].encode(clip_model, negative_prompt)[0]\n",
        "        latent = nodes[\"empty\"].generate(w, h, batch_size=1)[0]\n",
        "\n",
        "        # Sample\n",
        "        sample = nodes[\"sampler\"].sample(\n",
        "            unet_model, gen_seed, steps, guidance_scale,\n",
        "            \"euler\", \"simple\", pos_enc, neg_enc, latent, denoise=1.0\n",
        "        )[0]\n",
        "\n",
        "        # Decode\n",
        "        decoded = nodes[\"decode\"].decode(vae_model, sample)[0].detach()\n",
        "\n",
        "    # Save & Show\n",
        "    img_out = Image.fromarray(np.array(decoded * 255, dtype=np.uint8)[0])\n",
        "    save_path = get_save_path(positive_prompt)\n",
        "    img_out.save(save_path)\n",
        "\n",
        "    print(f\"\\033[92m‚úì Saved to: {save_path.name}\\033[0m\")\n",
        "    display(img_out)\n",
        "\n",
        "    if auto_download:\n",
        "        from google.colab import files\n",
        "        files.download(str(save_path))\n",
        "\n",
        "except NameError:\n",
        "    log(\"Engine not loaded. Please run Step 2.\", \"error\")\n",
        "except Exception as e:\n",
        "    log(f\"Error: {e}\", \"error\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}